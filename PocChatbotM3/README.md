# Chatbot RAG M3 ‚Äì Ultra-Avanc√©

Ce projet impl√©mente un **chatbot de nouvelle g√©n√©ration** utilisant l'architecture **RAG (Retrieval-Augmented Generation)** pour r√©pondre aux questions bas√©es sur la documentation interne M3 (PDF/DOCX/CSV).
Le syst√®me s'appuie sur **Mistral AI** (embeddings & chat) et **Faiss** (indexation vectorielle) via une interface **Streamlit**.

---

## üöÄ Architecture RAG Avanc√©e

Le pipeline RAG a √©t√© optimis√© pour un **RAG conversationnel** complet :

1. **Pr√©-traitement de la Requ√™te (`MistralChat.py`)**

   * Utilisation d'un LLM pour **r√©√©crire les questions de suivi** en les contextualisant avec l‚Äôhistorique complet du chat.
   * Exemple : "Quel est son prix‚ÄØ?" devient "Quel est le prix de [produit mentionn√© pr√©c√©demment]‚ÄØ?".

2. **Routage Hybride (`QueryClassifier.py`)**

   * D√©cision intelligente d'activer ou non la recherche RAG.
   * Bas√© sur des **mots-cl√©s sp√©cifiques** et un **seuil de similarit√© s√©mantique** pour √©viter de solliciter le LLM inutilement.

3. **Recherche Am√©lior√©e (`VectorStore.py`)**

   * R√©cup√©ration des **chunks pertinents** depuis l'index Faiss.
   * **Post-traitement contextuel** : inclusion des fragments voisins pour fournir un contexte plus riche et r√©duire les hallucinations.

---

## üõ†Ô∏è Pr√©requis

* Python **3.8+**
* Une **cl√© API Mistral AI** (`MISTRAL_API_KEY`)
* **Acc√®s √† la VM Azure** o√π le chatbot sera ex√©cut√©

---

## ‚öôÔ∏è Installation et Configuration

### 1. D√©marrage de la VM Azure

1. Connectez-vous au **[Azure Portal](https://portal.azure.com)**.
2. S√©lectionnez votre **VM**.
3. Cliquez sur **D√©marrer**.
4. Attendez environ **5 minutes** pour que la VM soit compl√®tement op√©rationnelle.

### 2. Cloner le d√©p√¥t et cr√©er l'environnement

```bash
git clone <URL_DU_DEPOT>
cd PocChatbotM3
python3 -m venv venv       # Windows : python -m venv venv
source venv/bin/activate   # Windows : venv\Scripts\activate
```

### 3. Installer les d√©pendances

```bash
pip install -r requirements.txt
# ou installation manuelle
pip install streamlit mistralai faiss-cpu pandas pypdf2 python-docx sqlalchemy python-dotenv langchain streamlit-feedback
```

### 4. Configurer les variables d'environnement

Cr√©ez un fichier `.env` √† la racine du projet :

```env
MISTRAL_API_KEY="votre_cl√©_mistral_ici"
```

### 5. Placer la documentation

Placez tous vos fichiers (PDF, DOCX, CSV, etc.) dans le dossier d√©fini dans `utils/config.py` (par d√©faut : `./inputs`).

---

## üß± Indexation des Documents

Avant de lancer le chatbot, construisez l'index vectoriel :

```bash
python indexer.py --input-dir inputs
```

**Description :**

* Lecture de tous les documents
* D√©coupage en **fragments (chunks)**
* G√©n√©ration des **embeddings** via Mistral AI
* Sauvegarde de l‚Äô**index Faiss** (`vector_db/faiss_index.idx`) et des fragments (`vector_db/document_chunks.pkl`)

---

## üñ•Ô∏è Lancement du Chatbot

```bash
streamlit run MistralChat.py
```

Acc√©dez √† l‚Äôapplication via votre navigateur : `http://localhost:8501`

---

## üìÇ Structure des Fichiers Cl√©s

| Fichier                         | R√¥le                                                           |
| ------------------------------- | -------------------------------------------------------------- |
| `MistralChat.py`                | Interface utilisateur Streamlit et moteur RAG principal        |
| `indexer.py`                    | Script de cr√©ation initiale de l'index vectoriel               |
| `utils/vector_store.py`         | Gestion des embeddings, Faiss et post-traitement du contexte   |
| `utils/query_classifier.py`     | Routage hybride : d√©cision RAG vs. Direct                      |
| `utils/conversation_history.py` | Gestion de l‚Äôhistorique des sessions et du RAG conversationnel |
| `utils/database.py`             | ORM SQLAlchemy pour logging et d√©bogage avanc√©                 |
